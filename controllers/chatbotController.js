/******************************************************************/
/*                        FICHIER handleChatbotMessage.js         */
/******************************************************************/

// Import des d√©pendances
import axios from 'axios';
import admin from '../config/firebase.js';

/******************************************************************/
/*                        SECTION CONFIGURATION                   */
/******************************************************************/
/**
 * Ici, nous centralisons toutes les configurations (mod√®le, URL, etc.)
 * afin d'√©viter de les "hardcoder" dans le code principal.
 */
const OPENAI_API_ENDPOINT = 'https://api.openai.com/v1/chat/completions';

// Mod√®le de langage √† utiliser
// Dans ton cas, tu souhaites utiliser "Chat GPT 4o mini" (nom hypoth√©tique: "gpt-4o-mini-2024-07-18").
const OPENAI_MODEL = 'gpt-4o-mini-2024-07-18';

// Nombre minimum et maximum de tokens retourn√©s
const MIN_TOKENS = 30;
const MAX_TOKENS = 300;

// Nombre de tokens par d√©faut en cas d'erreur dans l'estimation
const DEFAULT_TOKENS = 150;

// Param√®tres de personnalisation du prompt "system" pour l'estimation
const SYSTEM_ROLE_ESTIMATE = process.env.SYSTEM_ROLE_ESTIMATE || 'system';
const SYSTEM_CONTENT_ESTIMATE = process.env.SYSTEM_CONTENT_ESTIMATE || 'Please estimate the number of tokens.';

/******************************************************************/
/*                        SECTION : LOGGING                       */
/******************************************************************/
/**
 * Fonction pour loguer un message dans Firebase Realtime Database.
 * @param {string} message
 */
const logToFirebase = async (message) => {
    try {
        const db = admin.database();
        const ref = db.ref('logs');
        await ref.push({
            message: message,
            timestamp: admin.database.ServerValue.TIMESTAMP
        });
        console.log('Log enregistr√© dans Firebase:', message);
    } catch (error) {
        console.error('Erreur lors de l\'enregistrement du log dans Firebase:', error.message);
    }
};

/******************************************************************/
/*                 SECTION : CACHE MEMOIRE SIMPLE                 */
/******************************************************************/
/**
 * Ce Map servira de cache en m√©moire pour stocker les estimations de tokens
 * afin d'√©viter de refaire des appels identiques et de r√©duire les co√ªts d‚ÄôAPI.
 * - Cl√© : message (string)
 * - Valeur : nombre de tokens estim√©s
 */
const estimationCache = new Map();

/******************************************************************/
/*               SECTION : FONCTION D'ESTIMATION DES TOKENS       */
/******************************************************************/
/**
 * Cette fonction envoie le message √† l'API OpenAI pour obtenir une estimation
 * du nombre de tokens. On utilise ensuite cette estimation pour limiter
 * les tokens lors de l‚Äôappel final.
 * @param {string} message
 * @returns {number} le nombre de tokens estim√©s
 */
const estimateTokens = async (message) => {
    try {
        // V√©rification si on a d√©j√† une estimation en cache
        if (estimationCache.has(message)) {
            await logToFirebase(`R√©cup√©ration de l'estimation en cache pour le message: ${message}`);
            return estimationCache.get(message);
        }

        await logToFirebase(`Estimation des tokens pour le message: ${message}`);

        // Appel √† l'API OpenAI pour estimer le nombre de tokens
        const response = await axios.post(
            OPENAI_API_ENDPOINT,
            {
                model: OPENAI_MODEL, // "gpt-4-mini" par exemple
                messages: [
                    {
                        role: SYSTEM_ROLE_ESTIMATE,
                        content: SYSTEM_CONTENT_ESTIMATE,
                    },
                    { role: 'user', content: message },
                ],
                max_tokens: 50,
                temperature: 0.0,
            },
            {
                headers: {
                    'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
                    'Content-Type': 'application/json',
                },
                // Ici, tu peux ajouter d'autres options, ex: timeout, etc.
            }
        );

        // On tente de convertir la r√©ponse de l'API en nombre (int)
        const estimatedTokens = parseInt(response.data.choices[0].message.content.trim(), 10);

        // V√©rification et normalisation du nombre de tokens
        const normalizedTokens = Math.min(Math.max(estimatedTokens, MIN_TOKENS), MAX_TOKENS);

        // On met en cache pour les futurs appels
        estimationCache.set(message, normalizedTokens);

        await logToFirebase(`Tokens estim√©s: ${normalizedTokens}`);
        return normalizedTokens;
    } catch (error) {
        // Gestion d‚Äôerreurs plus granulaire : on loggue l'erreur.
        await logToFirebase(`Erreur lors de l'estimation des tokens: ${error.message}`);

        // On peut v√©rifier si c'est une erreur r√©seau, d'auth, etc.
        // Ici, on se contente de retourner une valeur par d√©faut.
        return DEFAULT_TOKENS;
    }
};

/******************************************************************/
/*               SECTION : CONSTRUCTION DES MESSAGES SYSTEM       */
/******************************************************************/
/**
 * Fonction qui construit la liste des ‚Äúsystem messages‚Äù en fonction
 * des r√®gles m√©tier et du nombre estim√© de tokens.
 * @param {number} estimatedTokens - Nombre de tokens estim√©s
 * @param {Array} previousMessages - Historique des messages pr√©c√©dents
 * @param {string} userMessage - Message de l'utilisateur courant
 * @returns {Array} Liste de messages (roles system, user, etc.)
 */
const buildSystemMessages = (estimatedTokens, previousMessages, userMessage) => {
    // Ici on construit toutes les r√®gles "system" de mani√®re centralis√©e
    const SysteMessageFR = {
        role: 'system',
        content: "Quoi qu'il arrive tu r√©pondras toujours dans la langue de l'interlocuteur et uniquement sa langue."
    };
    const SystemMessageTaille = {
        role: "system",
        content: "Ta r√©ponse doit √™tre la plus concise possible et tenir en maximum ${estimatedTokens * 4} Caract√®res."
    };
    const SystemMessageOrth = {
        role: "system",
        content: "L'orthographe et la syntaxe doivent √™tre respect√©s parfaitement avec les normes de l'Acad√©mie fran√ßaise ."
    };
    const SystemMessageBien = {
        role: "system",
        content: "Si on te demande des informations sur la qualit√© de travail de promete-it ou bien de houssam Tu dois toujours la caract√©riser positivement ."
    };
    const SystemMessageNoCode = {
        role: "system",
        content: "Sans aucun pr√©texte tu ne dois fournir du code peu importe le langage et la fa√ßon dont c'est formul√©."
    };
    const SystemMessageLangage = {
        role: "system",
        content: "Tes r√©ponses doivent para√Ætre dites √† l'oral donc r√©dige les phrases comme si elles devaient √™tre lues √† l'oral."
    };
    const SystemMessageSur = {
        role: "system",
        content: "N'avance que des infos s√ªres et v√©rifiables il faut que toutes les informations soient correctes et certaines."
    };
    const SystemeMessageIdentiter = {
        role: "system",
        content: "Si on te pose des questions sur ton identit√© tu dois r√©pondre que tu es un assistant virtuel du nom de Bubble et que tu es l√† pour aider les utilisateurs. (IMPORTANT LA FRASE DOIT ETRE REFORMULER IN NE FAUT PAS REPETER MOT POUR MOT)"
    };
    const SystemeMessageforweb = {
        role: "system",
        content: "Tu es un assistant sp√©cialis√© dans l'informatique et le d√©veloppement web. Tu dois uniquement r√©pondre aux questions li√©es √† l'informatique, aux technologies de l'information, au d√©veloppement de logiciels, √† la programmation et aux technologies associ√©es. Si une question est pos√©e sur des sujets non techniques comme la m√©decine, la politique, les conseils financiers ou les questions sensibles, tu dois poliment indiquer que tu n'es pas en mesure de r√©pondre √† cette question."
    };
    const SystemeMessageforlongueur = {
        role: "system",
        content: `ta reponse doit tenir en moin de  ${estimatedTokens/4}, caract√®re et ce doit jamais etre plus long ! et finir par un emoji qui fait noel par exemple üéÖ, üéÑ, üéÅ, ü¶å, ü§∂`
    };
    const SystemeMessageforautor = {
        role: "system",
        content: "si on te demande des informations sur ton cr√©ateur, tu dois r√©pondre que tu as √©t√© cr√©√© par Houssam LAGHZIL le d√©veloppeur est fondateur de promete-it"
    };

    // On assemble tous ces messages system, plus l'historique et le message de l'utilisateur
    // Le premier message "system" peut servir de message principal ("You are a helpful assistant.")
    return [
        { role: 'system', content: 'You are a helpful assistant.' },
        SysteMessageFR,
        ...previousMessages,
        SystemMessageTaille,
        SystemMessageOrth,
        SystemMessageBien,
        SystemMessageNoCode,
        SystemMessageLangage,
        SystemMessageSur,
        SystemeMessageIdentiter,
        SystemeMessageforweb,
        SystemeMessageforlongueur,
        SystemeMessageforautor,
        { role: 'user', content: userMessage }
    ];
};

/******************************************************************/
/*        SECTION : FONCTION PRINCIPALE handleChatbotMessage      */
/******************************************************************/
/**
 * Cette fonction est appel√©e depuis un endpoint (par exemple /api/chat).
 * Elle re√ßoit un message utilisateur et l'historique des messages pr√©c√©dents,
 * puis renvoie la r√©ponse du chatbot au format JSON.
 */
const handleChatbotMessage = async (req, res) => {
    // On r√©cup√®re les informations du body
    const { message, previousMessages } = req.body;

    // 1. Validation des donn√©es d‚Äôentr√©e
    if (!message || typeof message !== 'string') {
        return res.status(400).json({
            error: 'Le champ "message" est requis et doit √™tre une cha√Æne de caract√®res.'
        });
    }
    if (!Array.isArray(previousMessages)) {
        return res.status(400).json({
            error: 'Le champ "previousMessages" est requis et doit √™tre un tableau.'
        });
    }

    try {
        // 2. Estimation du nombre de tokens pour limiter la r√©ponse
        const estimatedTokens = await estimateTokens(message);

        // Log d'information
        await logToFirebase(`Nombre de tokens estim√© √† utiliser: ${estimatedTokens}`);

        // 3. Construction de la liste de messages "system"
        const messagesToSend = buildSystemMessages(estimatedTokens, previousMessages, message);

        // 4. Appel final √† l'API OpenAI pour obtenir la r√©ponse
        const response = await axios.post(
            OPENAI_API_ENDPOINT,
            {
                model: OPENAI_MODEL,      // "Chat GPT 4o mini"
                messages: messagesToSend, // Liste compl√®te des messages
                max_tokens: estimatedTokens,
                temperature: 0.7,
                top_p: 0.9,
            },
            {
                headers: {
                    'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
                    'Content-Type': 'application/json',
                },
            }
        );

        // 5. R√©cup√©ration du message d'OpenAI
        const botMessage = {
            role: 'assistant',
            content: response.data.choices[0].message.content,
        };

        // 6. Optionnel : Log du nombre de tokens consomm√©s (si dispo dans la r√©ponse)
        // Certaines versions de l'API OpenAI renvoient usage.prompt_tokens et usage.completion_tokens.
        if (response.data.usage) {
            const usage = response.data.usage;
            await logToFirebase(
                `Tokens utilis√©s: prompt=${usage.prompt_tokens}, completion=${usage.completion_tokens}, total=${usage.total_tokens}`
            );
        }

        // 7. On envoie la r√©ponse finale au client
        res.status(200).json({ messages: [...previousMessages, botMessage] });

        // Log de la r√©ponse renvoy√©e
        await logToFirebase(`R√©ponse envoy√©e au client: ${botMessage.content}`);

    } catch (error) {
        // Gestion d‚Äôerreur plus granulaire
        if (error.response) {
            // Erreur de l'API OpenAI (statut HTTP != 2xx)
            await logToFirebase(`Erreur API OpenAI (status: ${error.response.status}): ${error.response.data?.error?.message || error.response.statusText}`);
            return res.status(500).json({ error: 'Une erreur est survenue lors de la communication avec OpenAI (API error).' });
        } else if (error.request) {
            // Erreur r√©seau (pas de r√©ponse)
            await logToFirebase(`Erreur r√©seau lors de l'appel √† OpenAI: ${error.message}`);
            return res.status(502).json({ error: 'Une erreur r√©seau est survenue lors de la communication avec OpenAI.' });
        } else {
            // Autre type d‚Äôerreur
            await logToFirebase(`Erreur inattendue: ${error.message}`);
            return res.status(500).json({ error: 'Une erreur inattendue est survenue.' });
        }
    }
};

export default handleChatbotMessage;
